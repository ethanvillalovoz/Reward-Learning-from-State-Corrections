# Reward Learning from State Corrections

This repository contains the research materials, presentation, and other artifacts from my work conducted as part of the **CMU Robotics Institute Summer Scholars (RISS) Program 2024**. The project explores **reward learning from state corrections** in human-robot interaction, focusing on how robots can leverage clarification questions to align their actions with human preferences. 

---

## üöÄ Project Overview

As autonomous systems become increasingly prevalent, aligning robotic actions with human preferences is crucial for their utility and acceptance. Traditional methods such as teleoperation are often impractical in dynamic environments. Instead, this project investigates a novel approach where robots learn from **state corrections** provided by users. The key contributions of this work include:

- **Learning from Corrections:** Robots use human state corrections to improve task performance and align with user preferences.
- **Proactive Dialogue:** Robots ask **clarification questions** when uncertainty arises, improving efficiency and reducing errors.
- **Probabilistic Modeling:** Bayesian inference is used to update robot beliefs about user preferences iteratively.

---

## üìú Key Files and Resources

### 1. Presentation
- **`Lab_Final_Presentation.pdf`**  
  A concise slide deck summarizing the project's goals, methods, and preliminary results.

### 2. Poster
- **`RISS_Poster.pdf`**, **`RISS_Poster.pptx`**, and **`RISS_Poster.key`**  
  A research poster highlighting the project's contributions, including interactive workflows and results.

### 3. Research Paper
- **`RISS_Research_Paper.pdf`**  
  The full paper detailing the theoretical framework, methodology, and future work.

### 4. Project Video
- **`RISS_Video.mp4`**  
  A visual demonstration of the project's key workflows and outcomes.

---

## üéØ Key Contributions

- **Interactive Human-Robot Collaboration:** Developed a reward learning framework where robots refine actions through state corrections.
- **Clarification Questions:** Implemented a proactive dialogue mechanism for resolving feature ambiguity in user corrections.
- **Bayesian Inference:** Utilized probabilistic models to iteratively update robot beliefs about user preferences.

---

## üîç Future Work

- Enhance the generation of clarification questions using advanced large language models (LLMs).
- Conduct user studies to validate findings in real-world scenarios.
- Apply the methodology to other domains requiring close human-robot collaboration.

---

## üõ†Ô∏è How to Use This Repository

1. **Explore Research Materials:** Navigate to the `Research_Materials` folder for presentations, posters, and the full research paper.
2. **Watch the Demo:** View the `RISS_Video.mp4` in the `Media` folder to see the project in action.
3. **Learn the Approach:** Dive into the research paper for a detailed explanation of the methods and results.

---

## üëè Acknowledgments

This work was conducted as part of the **CMU Robotics Institute Summer Scholars (RISS) Program 2024**. I would like to thank:
- **Dr. Henny Admoni** and **Dr. Reid Simmons** for their guidance and support.
- **Michelle Zhao** for her invaluable mentorship.
- **Rachel Burcin** and **Dr. John Dolan** for leading the RISS Program and providing this incredible opportunity.

---

## üìù License

This project is licensed under the [MIT License](LICENSE). Feel free to use and adapt the materials with appropriate attribution.

---

## üí¨ Contact

For any questions or collaborations, feel free to reach out:
- **Email:** ethan.villalovoz@wsu.edu  
- **GitHub:** [ethanvillalovoz](https://github.com/ethanvillalovoz)
